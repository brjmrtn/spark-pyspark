{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark SQL trabaja con DataFrames.** Un DataFrame es una **representación relacional de los datos.** Proporciona funciones con capacidades similares a SQL. Además, permite escribir **consultas tipo SQL** para nuestro análisis de datos.\n",
    "\n",
    "Los DataFrames son similares a las tablas relacionales o DataFrames en Python / R aunque con muchas optimizaciones que se ejecutan de manera \"oculta\" para el usuario. Hay varias formas de crear DataFrames a partir de colecciones, tablas HIVE, tablas relacionales y RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear la sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (8, \"HHH\", \"dept3\", 3700),\n",
    "    (9, \"III\", \"dept3\", 4500),\n",
    "    (10, \"JJJ\", \"dept5\", 3400)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, ['id','name','dept','salary'])\n",
    "deptdf = spark.createDataFrame(dept, ['id', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  8| HHH|dept3|  3700|\n",
      "|  9| III|dept3|  4500|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un df a partir de una tabla de Hive\n",
    "df = spark.table('tbl_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones básicas en DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count\n",
    "* Cuenta el número de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns\n",
    "* Muestra las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'dept', 'salary']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtypes\n",
    "* Accede al DataType de columnas dentro del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('name', 'string'),\n",
       " ('dept', 'string'),\n",
       " ('salary', 'bigint')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schema\n",
    "* Comprueba cómo Spark almacena el esquema de DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,LongType,true),StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,LongType,true)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select\n",
    "* Selecciona columnas del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| AAA|\n",
      "|  2| BBB|\n",
      "|  3| CCC|\n",
      "|  4| DDD|\n",
      "|  5| EEE|\n",
      "|  6| FFF|\n",
      "|  7| GGG|\n",
      "|  8| HHH|\n",
      "|  9| III|\n",
      "| 10| JJJ|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "* Filtrar las filas según alguna condición\n",
    "* Intentemos encontrar las filas con el id = 1\n",
    "* Hay diferentes formas de especificar la condición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n",
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['id'] == 1).show()\n",
    "df.filter(df.id== 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n",
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('id') == 1).show()\n",
    "df.filter('id =1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop\n",
    "* Elimina una columna en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| AAA|dept1|  1000|\n",
      "| BBB|dept1|  1100|\n",
      "+----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.drop('id')\n",
    "newdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "* Podemos usar la función groupBy para agrupar los datos y luego usar la función \"agg\" para realizar la agregación de datos agrupados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+----+----+------+\n",
      "| dept|count|  sum| max| min|   avg|\n",
      "+-----+-----+-----+----+----+------+\n",
      "|dept5|    1| 3400|3400|3400|3400.0|\n",
      "|dept3|    3|15300|7100|3700|5100.0|\n",
      "|dept1|    4| 6600|3000|1000|1650.0|\n",
      "|dept2|    2|15200|8000|7200|7600.0|\n",
      "+-----+-----+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy(\"dept\")\n",
    "    .agg(\n",
    "        count(\"salary\").alias(\"count\"),\n",
    "        sum(\"salary\").alias(\"sum\"),\n",
    "        max(\"salary\").alias(\"max\"),\n",
    "        min(\"salary\").alias(\"min\"),\n",
    "        avg(\"salary\").alias(\"avg\")\n",
    "        ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "* Ordena los datos según el \"salario\". De forma predeterminada, la clasificación se realizará en orden ascendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  3| CCC|dept1|  3000|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"salary\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  9| III|dept3|  4500|\n",
      "|  8| HHH|dept3|  3700|\n",
      "+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort the data in descending order\n",
    "df.sort(desc('salary')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columnas derivadas\n",
    "* Podemos usar la función \"withColumn\" para derivar la columna en función de las columnas existentes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+\n",
      "| id|name| dept|salary|bonus|\n",
      "+---+----+-----+------+-----+\n",
      "|  1| AAA|dept1|  1000|100.0|\n",
      "|  2| BBB|dept1|  1100|110.0|\n",
      "|  3| CCC|dept1|  3000|300.0|\n",
      "|  4| DDD|dept1|  1500|150.0|\n",
      "|  5| EEE|dept2|  8000|800.0|\n",
      "|  6| FFF|dept2|  7200|720.0|\n",
      "|  7| GGG|dept3|  7100|710.0|\n",
      "|  8| HHH|dept3|  3700|370.0|\n",
      "|  9| III|dept3|  4500|450.0|\n",
      "| 10| JJJ|dept5|  3400|340.0|\n",
      "+---+----+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('bonus', col('salary') * .1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "* Podemos realizar varios tipos de combinaciones en multiples DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+--------------+\n",
      "| id|name| dept|salary|   id|          name|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|  9| III|dept3|  4500|dept3|Department - 3|\n",
      "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df['dept'] == deptdf['id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+--------------+\n",
      "| id|name| dept|salary|   id|          name|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "| 10| JJJ|dept5|  3400| null|          null|\n",
      "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|  9| III|dept3|  4500|dept3|Department - 3|\n",
      "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df['dept'] == deptdf['id'], 'left_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n",
      "|  id|name| dept|salary|   id|          name|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|   9| III|dept3|  4500|dept3|Department - 3|\n",
      "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|null|null| null|  null|dept4|Department - 4|\n",
      "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df['dept'] == deptdf['id'], 'right_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n",
      "|  id|name| dept|salary|   id|          name|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "|  10| JJJ|dept5|  3400| null|          null|\n",
      "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|   9| III|dept3|  4500|dept3|Department - 3|\n",
      "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|null|null| null|  null|dept4|Department - 4|\n",
      "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df['dept'] == deptdf['id'], 'outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultas SQL\n",
    "* Ejecución de consultas tipo SQL\n",
    "* También podemos realizar análisis de datos escribiendo consultas similares a SQL. Para realizar consultas similares a SQL, necesitamos registrar el DataFrame como una Vista temporal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as Temporary Table\n",
    "df.createOrReplaceTempView('temp_table')\n",
    "\n",
    "# Execute SQL-Like query\n",
    "spark.sql('select * from temp_table where id = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  6|\n",
      "|  9|\n",
      "|  5|\n",
      "|  1|\n",
      "| 10|\n",
      "|  3|\n",
      "|  8|\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select distinct id from temp_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  8| HHH|dept3|  3700|\n",
      "|  9| III|dept3|  4500|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from temp_table where salary >= 1500').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leyendo la tabla HIVE como DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB_NAME : Name of the HIVE Database\n",
    "# TBL_NAME : Name of the HIVE Table\n",
    "df = spark.table('DB_NAME'.'TBL_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar DataFrame como tabla HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"DB_NAME.TBL_NAME\")\n",
    "\n",
    "## También podemos seleccionar el argumento \"modo\" con overwrite\", \"append\", \"error\" etc.\n",
    "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", mode=\"overwrite\")\n",
    "\n",
    "# De forma predeterminada, la operación guardará el DataFrame como una tabla interna / administrada de HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el DataFrame como una tabla externa HIVE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea un DataFrame a partir de un archivo CSV\n",
    "* Podemos crear un DataFrame usando un archivo CSV y podemos especificar varias opciones como un separador, encabezado, esquema, inferSchema y varias otras opciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = spark.read.csv(\"path_to_csv_file\",\n",
    "                     sep=\"|\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar un DataFrame como un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('path_to_CSV_file', sep=\"|\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea un DataFrame a partir de una tabla relacional\n",
    "* Podemos leer los datos de bases de datos relacionales usando una URL JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
    "# TBL_NAME : Name of the relational table.\n",
    "# USER_NAME : user name to connect to DataBase.\n",
    "# PASSWORD: password to connect to DataBase.\n",
    "\n",
    "\n",
    "relational_df = spark.read.format('jdbc')\n",
    "                        .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
    "                        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el DataFrame como una tabla relacional\n",
    "* Podemos guardar el DataFrame como una tabla relacional usando una URL JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
    "# TBL_NAME : Name of the relational table.\n",
    "# USER_NAME : user name to connect to DataBase.\n",
    "# PASSWORD: password to connect to DataBase.\n",
    "\n",
    "\n",
    " relational_df.write.format('jdbc')\n",
    "                    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
    "                    .mode('overwrite')\n",
    "                    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
