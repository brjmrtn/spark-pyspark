{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de Apache Spark: RDDS\n",
    "\n",
    "En este notebook trabajaremos con los RDDs que forma parte del Spark Core.La implementación de Spark Core es un RDD (Resilient Distributed Dataset) que es una colección de datos distribuidos en diferentes nodos del clúster que se procesan en paralelo.\n",
    "\n",
    "Utilizaremos la API de PySpark, pero los conceptos aplican por igual a todas las APIs (Scala, R, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INICIALIZACIÓN DE SPARK EN NOTEBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debemos instalar findspark si no la tenemos\n",
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark    # importamos findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd # importamos pandas y pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # De la libreria pyspark.sql importamos SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREAR EL SPARKSESSION Y EL SPARKCONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('PySpark_training')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra forma de crear SparkSession de forma más sencilla\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un RDD de una colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = [1,2,3,4,5]\n",
    "num_rdd = sc.parallelize(num)\n",
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones\n",
    "* Como sabemmos, las transformaciones son de naturaleza perezosa y no se ejecutaran hasta que se ejecute una Acción sobre ellas\n",
    "* Intentemos comprender las distintas transformaciones disponobles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map\n",
    "* Esto mapeará su entrada a alguuna salida basada en la función especificada en la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_rdd = num_rdd.map(lambda x: x*2)\n",
    "double_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter\n",
    "* Para filtrar los datos en función de una determinada condición. Intentemos encontrar los números pares de num_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_rdd = num_rdd.filter(lambda x : x % 2 == 0)\n",
    "even_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flatmap\n",
    "* Esta función reduce los pares de valores clave en función de las claves y una funcion determinada dentro de reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b', 8)\n",
      "('a', 12)\n",
      "('c', 6)\n"
     ]
    }
   ],
   "source": [
    "pairs = [('a', 5), ('b', 7), (\"c\", 2),(\"a\", 7),(\"b\", 1),(\"c\", 4)]\n",
    "pair_rdd = sc.parallelize(pairs)\n",
    "\n",
    "output = pair_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "result = output.collect()\n",
    "print(*result, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distinct\n",
    "* Esto devolverá elementos distintos de un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 10, 11]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([10, 11, 10, 11, 12, 11])\n",
    "dist_rdd = rdd1.distinct()\n",
    "dist_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduceByKey\n",
    "* Esta función reduce los pares de valores clave en función de las claves y una función determinada dentro de reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b', 8)\n",
      "('a', 8)\n",
      "('c', 6)\n"
     ]
    }
   ],
   "source": [
    "pairs = [ (\"a\", 5), (\"b\", 7), (\"c\", 2), (\"a\", 3), (\"b\", 1), (\"c\", 4)]\n",
    "pair_rdd = sc.parallelize(pairs)\n",
    "\n",
    "output = pair_rdd.reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "result = output.collect()\n",
    "print(*result, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupByKey\n",
    "* Esta función es otra función ByKey que puede operar en un par (clave, valor) RDD pero esto solo agrupará los valores basados en las claves. En otras palabras, esto solo lo realizará el primer paso de reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', <pyspark.resultiterable.ResultIterable at 0x2370891f1c8>),\n",
       " ('a', <pyspark.resultiterable.ResultIterable at 0x2370891f508>),\n",
       " ('c', <pyspark.resultiterable.ResultIterable at 0x23707ef1ec8>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_out = pair_rdd.groupByKey()\n",
    "grp_out.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortByKey\n",
    "* Esta función realizará la clasificación en un par (clave, valor) RDD basado en las claves. De forma predeterminada, la clasificación se realizará en orden ascendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 5)\n",
      "('b', 3)\n",
      "('c', 2)\n",
      "('d', 7)\n"
     ]
    }
   ],
   "source": [
    "pairs =  [ (\"a\", 5), (\"d\", 7), (\"c\", 2), (\"b\",3)]\n",
    "raw_rdd = sc.parallelize(pairs)\n",
    "\n",
    "sortkey_rdd = raw_rdd.sortByKey()\n",
    "result = sortkey_rdd.collect()\n",
    "print(result, sep='\\n')\n",
    "\n",
    "# Para clasificar en orden descendente, determinar \"ascending=Flase\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortBy\n",
    "* es una función mas generalizada para ordenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 3, 9), ('a', 5, 10), ('c', 2, 11), ('d', 7, 12)]\n"
     ]
    }
   ],
   "source": [
    "#Create RDD\n",
    "pairs = [(\"a\", 5, 10), (\"d\", 7, 12), (\"c\", 2, 11), (\"b\", 3, 9)]\n",
    "raw_rdd = sc.parallelize(pairs)\n",
    "\n",
    "# Let's try to do the sorting based on the 3rd element of the tuple\n",
    "sort_out = raw_rdd.sortBy(lambda x : x[2])\n",
    "result = sort_out.collect()\n",
    "print(result, sep ='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones\n",
    "* Las acciones son operaciones en RDD que se ejecutan inmediatamente. Mientras que las transformaciones devuelven otro RDD, las acciones devuelven estructuras de datos nativas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### count\n",
    "* esto contará el número de elementos en el RDD dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = sc.parallelize([1,2,3,4,2])\n",
    "num.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first\n",
    "* Esto devolverá el primer elemento del RDD dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect\n",
    "* Esto devolverá todos los elementos para el RDD dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No debemos utiliza la operación de collect mientras trabajamos con grandes conjuntos de datos.** Porque devolverá todos los datos que se distribuyen entre los diferentes trabajadores del cluster a un controlador. Todos los datos viajarán a través de la red del trabajador al conductor y también el conducto necesitaria almacenar todos los datos. Esto obstaculizará el rendimiento de su aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take\n",
    "* Esto devolverá el número de elementos especificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ab003e543c44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "num.take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
